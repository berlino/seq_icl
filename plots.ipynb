{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = [\n",
    "\"outputs/2023-10-18/11-44-08-805221\",\n",
    "# \"outputs/2023-11-01/16-24-19-171848\",\n",
    "\"outputs/2023-10-18/11-44-08-874258\",\n",
    "\"outputs/2023-10-18/11-44-08-874396\",\n",
    "\"outputs/2023-10-18/11-44-08-882528\",\n",
    "\"outputs/2023-10-18/11-44-08-884445\",\n",
    "\"outputs/2023-10-18/11-44-08-886024\",\n",
    "\"outputs/2023-10-18/11-44-08-906468\",\n",
    "\"outputs/2023-10-18/11-44-08-911000\",\n",
    "\"outputs/2023-10-18/11-44-08-932201\",\n",
    "\"outputs/2023-10-18/11-44-08-953521\",\n",
    "# \"outputs/2023-11-12/19-20-15-010722\",\n",
    "\"outputs/2023-11-13/12-05-39-054691\",\n",
    "\"outputs/2023-11-14/10-27-23-673379\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths2 = [\n",
    "\"/raid/lingo/akyurek/git/iclmodels/outputs/2023-11-15/11-44-53-320622\",\n",
    "\"/raid/lingo/akyurek/git/iclmodels/outputs/2023-11-15/11-44-53-041944\",\n",
    "\"/raid/lingo/akyurek/git/iclmodels/outputs/2023-11-15/11-44-53-295893\",\n",
    "\"/raid/lingo/akyurek/git/iclmodels/outputs/2023-11-15/11-44-53-403698\",\n",
    "\"/raid/lingo/akyurek/git/iclmodels/outputs/2023-11-15/11-44-52-854931\",\n",
    "\"/raid/lingo/akyurek/git/iclmodels/outputs/2023-11-15/12-21-36-646480\",\n",
    "\"/raid/lingo/akyurek/git/iclmodels/outputs/2023-11-15/12-21-36-588119\",\n",
    "\"/raid/lingo/akyurek/git/iclmodels/outputs/2023-11-15/12-27-29-253904\",\n",
    "\"/raid/lingo/akyurek/git/iclmodels/outputs/2023-11-15/12-21-36-614857\",\n",
    "\"/raid/lingo/akyurek/git/iclmodels/outputs/2023-11-15/12-00-28-036885\",\n",
    "\"/raid/lingo/akyurek/git/iclmodels/outputs/2023-11-15/11-44-53-222033\",\n",
    "\"/raid/lingo/akyurek/git/iclmodels/outputs/2023-11-15/11-44-53-201063\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "from yaml import Loader\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from probe import get_results\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "exp_folders = {}\n",
    "for path in paths2:\n",
    "    yaml_file = os.path.join(path, \".hydra\", \"overrides.yaml\")\n",
    "    # parse file\n",
    "    with open(yaml_file, 'r') as stream:\n",
    "        try:\n",
    "            data = yaml.safe_load(stream)\n",
    "            print(data)\n",
    "            experiment = data[-1]\n",
    "            experiment = experiment.split(\"=\")[1]\n",
    "            experiment = experiment.replace(\"dfa/\", \"\")\n",
    "            if experiment == \"lstm\":\n",
    "                layer = data[4]\n",
    "            else:\n",
    "                layer = data[5]\n",
    "            layer = layer.split(\"=\")[1]\n",
    "            exp_folders[f\"{experiment}/{layer}\"] = path\n",
    "            print(\"export PYTHONHASHSEED=0; export CUDA_VISIBLE_DEVICES=0; python train.py model.return_attention=True\" + \" \".join(data))\n",
    "            # ckpt = glob.glob(os.path.join(path, \"finalexps2\", \"*/checkpoints/*.ckpt\"))[0]\n",
    "            # # escape equal sign\n",
    "            # ckpt = ckpt.replace(\"=\", \"\\\\=\")\n",
    "            # overrides = \" \".join(data)\n",
    "            # print(f\"export PYTHONHASHSEED=0; export CUDA_VISIBLE_DEVICES=2; python train.py train.ckpt=\\\"{ckpt}\\\" train.test=true {overrides} > newexps/{experiment}_{layer}.log 2> newexps/{experiment}_{layer}.err &\")\n",
    "        except yaml.YAMLError as exc:\n",
    "            print(exc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# video_files = \"\"\"outputs/2023-10-18/11-44-08-805221/res_lw_True_videos TF8\n",
    "# outputs/2023-10-18/11-44-08-874258/res_lw_True_videos TF2\n",
    "# outputs/2023-10-18/11-44-08-874396/res_lw_True_videos TF4\n",
    "# outputs/2023-10-18/11-44-08-884445/res_lw_True_videos LTF4\n",
    "# outputs/2023-10-18/11-44-08-886024/res_lw_True_videos RT4\n",
    "# outputs/2023-10-18/11-44-08-906468/res_lw_True_videos RWKV\n",
    "# outputs/2023-10-18/11-44-08-911000/res_lw_True_videos H3\n",
    "# outputs/2023-10-18/11-44-08-932201/res_lw_True_videos HYENA\n",
    "# outputs/2023-11-13/12-05-39-054691/res_lw_True_videos TF12\n",
    "# outputs/2023-11-14/10-27-23-673379/res_lw_True_videos LTF8\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # copy videos under the folders to a named folder\n",
    "# import os\n",
    "# import shutil\n",
    "# import glob\n",
    "# import re\n",
    "\n",
    "# os.makedirs(\"video_artifacts\", exist_ok=True)\n",
    "# for line in video_files.split(\"\\n\"):\n",
    "#     folder, name = line.split(\" \")\n",
    "#     video_folder = os.path.join(folder, \"res_lw_True_videos\")\n",
    "#     os.makedirs(\"video_artifacts/\" + name, exist_ok=True)\n",
    "#     # copy video folder to new named folder using cp\n",
    "#     os.system(f\"cp -r {video_folder} video_artifacts/{name}/\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In-context learning curve of each model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def in_context_learning_curve(folder, title=\"null\"):\n",
    "    data = get_results(folder + \"/generations/200_test.txt\")\n",
    "    accs = []\n",
    "    for t in range(300):\n",
    "        total = 0.0\n",
    "        correct = 0.0\n",
    "        for b in range(len(data)):\n",
    "            labels = data[b][\"char_labels\"]\n",
    "            if len(labels) > t:\n",
    "                correct += labels[t]\n",
    "                total += 1\n",
    "        accs.append(correct / total)\n",
    "    # plot accs\n",
    "    # make high res\n",
    "    if title in (\"transformer/4\", \"transformer/2\", \"transformer/1\"):\n",
    "        return None\n",
    "    title = title.split(\"/\")[0]\n",
    "    # make first letter uppercase\n",
    "    title = title[0].upper() + title[1:]\n",
    "    title = title.replace(\"_\", \" \")\n",
    "\n",
    "\n",
    "    plt.plot(accs, label=title)\n",
    "    # set x label\n",
    "    plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = get_results(exp_folders[\"transformer/8\"] + \"/generations/200_test.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data1[0][\"vocab\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data1[0][\"input\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_dfa_data(data):\n",
    "    for index, datum in enumerate(data):\n",
    "        input = datum[\"input\"]\n",
    "        examples = input.split(\"|\")\n",
    "        vocab = sorted(list(set(list(\"\".join(examples)))))\n",
    "        inv_vocab = {v: i for i, v in enumerate(vocab)}\n",
    "        with open(f\"FlexFringe/data/icl_dfa_data/{index}.dat\", \"w\") as handle:\n",
    "            print(f\"{len(examples)} {len(vocab)}\", file=handle)\n",
    "            for example in examples:\n",
    "                encoded = [inv_vocab[c] for c in example]\n",
    "                encoded_str = ' '.join([str(c) for c in encoded])\n",
    "                print(f\"1 {len(encoded)} {encoded_str}\", file=handle)\n",
    "        # print vocab information to another file with same index\n",
    "        # with open(f\"FlexFringe/data/icl_dfa_data/{index}_vocab.txt\", \"w\") as handle:\n",
    "        #     vocab_str = ' '.join(vocab)\n",
    "        #     print(f\"{vocab_str}\", file=handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_dfa_data(data1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = get_results(exp_folders[\"h3/2\"] + \"/generations/200_test.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d1, d2 in zip(data1, data2):\n",
    "    assert d1[\"input\"] == d2[\"input\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure matplotlib to make high-quality conference style\n",
    "rc = {\n",
    "    \"font.family\": \"sans-serif\",\n",
    "    \"font.sans-serif\": [\"Helvetica\"],\n",
    "    \"font.size\": 10,\n",
    "    \"axes.labelsize\": 12,\n",
    "    \"axes.titlesize\": 12,\n",
    "    \"legend.fontsize\": 10,\n",
    "    \"xtick.labelsize\": 10,\n",
    "    \"ytick.labelsize\": 10,\n",
    "    \"figure.figsize\": [3.39, 2.10],\n",
    "    \"figure.dpi\": 300,\n",
    "    \"text.usetex\": True,\n",
    "    \"text.latex.preamble\": r\"\\usepackage{amsmath}\\usepackage{amssymb}\",\n",
    "}\n",
    "# set rc\n",
    "plt.rcParams.update(rc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot learning curves\n",
    "plt.figure(figsize=(8, 6))\n",
    "# add xlabel\n",
    "plt.xlabel(\"number of characters in context\")\n",
    "# set y label\n",
    "plt.ylabel(\"next character validity\")\n",
    "# plt.title(\"In-context Accuracy\")\n",
    "for exp  in exp_folders.keys():\n",
    "    try:\n",
    "        in_context_learning_curve(exp_folders[exp], title=exp)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"skipping {exp}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## n-gram vs model; model vs dfa; dfa vs n-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from analyze import get_dfa_probs as calculate_dfa_probs\n",
    "from ngram import (\n",
    "    predict_with_n_gram_back_off,\n",
    "    prob_distance,\n",
    "    prob_distance_dfa,\n",
    "    prob_distance_dfa_ngram,\n",
    ")\n",
    "\n",
    "from batched_baum_welch import predict_with_baumwelch\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class Vocab:\n",
    "    def __init__(self, vocab: list):\n",
    "        self.vocab = vocab\n",
    "        # inverse vocab\n",
    "        self.inv_vocab = {v: k for k, v in enumerate(vocab)}\n",
    "\n",
    "    def get_vocab(self, id):\n",
    "        return self.vocab[id]\n",
    "\n",
    "    def get_id(self, char):\n",
    "        return self.inv_vocab[char]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.vocab)\n",
    "\n",
    "def get_ngram_probs(results, ngram=3, uniform=False, backoff=False, addone=False):\n",
    "    vocab = Vocab(results[0][\"vocab\"])\n",
    "    n_gram_probs = []\n",
    "    for b in range(len(results)):\n",
    "        input = results[b][\"input\"]\n",
    "        target = [vocab.get_id(t) for t in results[b][\"target\"]]\n",
    "        probs = predict_with_n_gram_back_off(input, N=ngram, global_vocab=vocab, uniform=uniform, backoff=backoff, addone=addone)\n",
    "        n_gram_probs.append(probs)\n",
    "    return n_gram_probs\n",
    "\n",
    "def get_baumwelch_probs(results):\n",
    "    vocab = Vocab(results[0][\"vocab\"])\n",
    "    baumwelch_probs = []\n",
    "    for b in range(len(results)):\n",
    "        input = results[b][\"input\"]\n",
    "        probs = predict_with_baumwelch(input, vocab, max_states=12)\n",
    "        baumwelch_probs.append(probs)\n",
    "    return baumwelch_probs\n",
    "\n",
    "def get_dfa_probs(results):\n",
    "    vocab = Vocab(results[0][\"vocab\"])\n",
    "    dfa_probs = []\n",
    "    for b in range(len(results)):\n",
    "        input = results[b][\"input\"]\n",
    "        target = [vocab.get_id(t) for t in results[b][\"target\"]]\n",
    "        probs = calculate_dfa_probs(input, results[b][\"dfa\"], vocab=vocab)\n",
    "        dfa_probs.append(probs)\n",
    "    return dfa_probs\n",
    "\n",
    "def get_model_probs(results):\n",
    "    model_probs = []\n",
    "    for b in range(len(results)):\n",
    "        probs = torch.softmax(torch.tensor(results[b][\"probs\"]), dim=-1).detach().cpu().numpy()\n",
    "        model_probs.append(probs)\n",
    "    return model_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def get_greedy_dfa_accuracy(probs, dfa_probs):\n",
    "    total = 0.0\n",
    "    correct = 0.0\n",
    "    for p1, pdfa in zip(probs, dfa_probs):\n",
    "        indices = p1.argmax(axis=-1)[:len(pdfa)]\n",
    "        correct += (pdfa[np.arange(len(pdfa)), indices] > 0).sum()\n",
    "        total += len(pdfa)\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPS=1e-7\n",
    "def get_cross_entropy(probs, dfa_probs):\n",
    "    total = 0.0\n",
    "    cross_entropy = 0.0\n",
    "    for p1, pdfa in zip(probs, dfa_probs):\n",
    "        # calculate the soft cross-entropy between p1 and pdfa\n",
    "        log_p1 = np.log(p1[:len(pdfa)] + EPS)\n",
    "        log_pdfa = np.log(pdfa + EPS)\n",
    "        cross_entropy += -((log_p1 - log_pdfa) * pdfa).sum()\n",
    "        total += len(pdfa)\n",
    "    return cross_entropy / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_l1_loss(probs1, probs2, probsdfa, offset=0, max_len=None):\n",
    "    total = 0.0\n",
    "    correct = 0.0\n",
    "    for p1, p2, pdfa in zip(probs1, probs2, probsdfa):\n",
    "        if max_len is not None:\n",
    "            pdfa = pdfa[offset:max_len]\n",
    "        total += len(pdfa)\n",
    "        correct += np.abs(p1[offset:offset + len(pdfa)] - p2[offset:offset + len(pdfa)]).sum()\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = get_results(exp_folders[\"transformer/8\"] + \"/generations/200_test.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs[\"dfa\"] = get_dfa_probs(data1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs[\"bw\"]  = get_baumwelch_probs(data1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_greedy_dfa_accuracy(probs[\"bw\"] , probs[\"dfa\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_cross_entropy(probs[\"bw\"] , probs[\"dfa\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs[\"3gram\"] = get_ngram_probs(data1, 3, uniform=False, addone=False, backoff=True)\n",
    "probs[\"2gram\"] = get_ngram_probs(data1, 2, uniform=False, addone=False, backoff=True)\n",
    "# probs[\"3gram2gram1\"] = [(0.7 * probs3 + 0.3 * probs2) for probs3, probs2 in zip(probs[\"3gram\"], probs[\"2gram\"])]\n",
    "# probs[\"3gram2gram2\"] = [(0.5 * probs3 + 0.5 * probs2) for probs3, probs2 in zip(probs[\"3gram\"], probs[\"2gram\"])]\n",
    "# probs[\"3gram2gram3\"] = [(0.3 * probs3 + 0.7 * probs2) for probs3, probs2 in zip(probs[\"3gram\"], probs[\"2gram\"])]\n",
    "probs[\"3gramU\"] = get_ngram_probs(data1, 3, uniform=True, addone=False, backoff=True)\n",
    "probs[\"2gramU\"] = get_ngram_probs(data1, 2, uniform=True, addone=False, backoff=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# probs.pop(\"3gram2gram1\")\n",
    "# probs.pop(\"3gram2gram2\")\n",
    "# probs.pop(\"3gram2gram3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_greedy_dfa_accuracy(probs[\"3gram\"] , probs[\"dfa\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# probs[\"3gram\"][0][200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for exp_name, folder in exp_folders.items():\n",
    "    print(exp_name)\n",
    "    try:\n",
    "        data = get_results(folder + \"/generations/200_test.txt\")[:20]\n",
    "        probs[exp_name] = get_model_probs(data)\n",
    "    except:\n",
    "        print(f\"skipping {exp_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logitsmap(current_probs, chars, vocab, model=\"transformer/8\", id=0, offset=200):\n",
    "    # l1_losses is a dict of (key1, key2) -> l1_loss\n",
    "    # we want to create a df where key1s are the index\n",
    "    # and key2s are the columns\n",
    "\n",
    "    plt.figure(figsize=(30, 20))\n",
    "\n",
    "    key1s = vocab\n",
    "    # key2s = list(map(str, enumerate(chars[200:300])))\n",
    "    chars = chars[offset:offset+100]\n",
    "    indices = list(range(offset, offset+len(chars)))\n",
    "    key2s = list(map(str, zip(indices, chars)))\n",
    "\n",
    "    plot_index = 0\n",
    "    ax1 = None\n",
    "    for name, prob in current_probs.items():\n",
    "        df = pd.DataFrame(index=key1s, columns=key2s)\n",
    "        for index, key2 in enumerate(key2s):\n",
    "            df.loc[:, key2] = prob[offset + index]\n",
    "        # replace nans with 0\n",
    "        df = df.fillna(0)\n",
    "        # heatmap\n",
    "        ax = plt.subplot(len(current_probs), 1, plot_index + 1)\n",
    "        if ax1 is None:\n",
    "            ax1 = ax\n",
    "            ax.xaxis.tick_top()\n",
    "        else:\n",
    "            # remove xticks\n",
    "            ax.xaxis.set_visible(False)\n",
    "\n",
    "        sns.heatmap(df, annot=False, ax=ax, vmin=0, vmax=1)\n",
    "\n",
    "        ax.sharex(ax1)\n",
    "\n",
    "\n",
    "        # add title\n",
    "        plt.title(f\"{name}\")\n",
    "        plot_index += 1\n",
    "\n",
    "    os.makedirs(f\"attention_plots/{model}\", exist_ok=True)\n",
    "    path = os.path.join(\"attention_plots\", f\"{model}\", f\"{id}_{offset}_{offset + len(chars)}_probs.png\")\n",
    "    plt.savefig(path, dpi=250)\n",
    "    return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for exp_name, folder in exp_folders.items():\n",
    "    print(exp_name)\n",
    "    try:\n",
    "        for index in range(2):\n",
    "            modelprobs = probs[exp_name][index]\n",
    "            gram3probs = probs[\"3gram\"][index]\n",
    "            gram2probs = probs[\"2gram\"][index]\n",
    "            dfaprobs = probs[\"dfa\"][index]\n",
    "            modelgram3diff = np.abs(modelprobs[:len(gram3probs)] - gram3probs)\n",
    "            modeldfadiff = np.abs(modelprobs[:len(dfaprobs)] - dfaprobs)\n",
    "            modelgram2diff = np.abs(modelprobs[:len(gram2probs)] - gram2probs)\n",
    "\n",
    "            currentprobs = {\n",
    "                exp_name: modelprobs,\n",
    "                \"3gram\": gram3probs,\n",
    "                \"2gram\": gram2probs,\n",
    "                \"DFA\": dfaprobs,\n",
    "                f\"|{exp_name} - 3gram|\": modelgram3diff,\n",
    "                f\"|{exp_name} - DFA|\": modeldfadiff,\n",
    "                f\"|{exp_name} - 2gram|\": modelgram2diff,\n",
    "            }\n",
    "\n",
    "            print(logitsmap(currentprobs, list(data[index]['input']), data[index]['vocab'], model=exp_name, id=f\"{index}_probs\"))\n",
    "            print(logitsmap(currentprobs, list(data[index]['input']), data[index]['vocab'], model=exp_name, id=f\"{index}_probs\", offset=0))\n",
    "\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(f\"skipping {exp_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies = {}\n",
    "for key1 in probs.keys():\n",
    "    try:\n",
    "        accuracies[key1] = get_greedy_dfa_accuracy(probs[key1], probs[\"dfa\"])\n",
    "    except:\n",
    "        print(f\"skipping {key1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ce_losses = {}\n",
    "for key1 in probs.keys():\n",
    "    try:\n",
    "        ce_losses[key1] = get_cross_entropy(probs[key1], probs[\"dfa\"])\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(f\"skipping {key1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ce_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot bar chart of ce_losses with labels\n",
    "\n",
    "plt.figure(figsize=(32, 10))\n",
    "plt.bar(ce_losses.keys(), ce_losses.values())\n",
    "plt.title(\"KL (pmodel, pdfa)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# similar bar chart for accuracy\n",
    "plt.figure(figsize=(32, 8))\n",
    "plt.bar(accuracies.keys(), accuracies.values())\n",
    "plt.title(\"Average Validity Accuracy\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1_losses = {}\n",
    "for key1 in probs.keys():\n",
    "    for key2 in probs.keys():\n",
    "        if key1 == key2:\n",
    "            continue\n",
    "        try:\n",
    "            l1_losses[(key1, key2)] = get_l1_loss(probs[key1], probs[key2], probs[\"dfa\"], offset=0, max_len=100)\n",
    "        except Exception as e:\n",
    "            print(f\"skipping {key1}, {key2}\")\n",
    "            print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def heatmap(l1_losses, title=\"\"):\n",
    "    # l1_losses is a dict of (key1, key2) -> l1_loss\n",
    "    # we want to create a df where key1s are the index\n",
    "    # and key2s are the columns\n",
    "    key1s = sorted(list({key1 for key1, key2 in l1_losses.keys()}))\n",
    "    key2s = sorted(list({key2 for key1, key2 in l1_losses.keys()}))\n",
    "    df = pd.DataFrame(index=key1s, columns=key2s)\n",
    "    for key1, key2 in l1_losses.keys():\n",
    "        df.loc[key1, key2] = l1_losses[(key1, key2)]\n",
    "    # replace nans with 0\n",
    "    df = df.fillna(0)\n",
    "    # heatmap\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(df, annot=True)\n",
    "    # add title\n",
    "    plt.title(f\"L1 Loss (pmodel1, pmodel2) {title}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmap(l1_losses);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmap(l1_losses, title=\"offset=200, max_len=300\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmap(l1_losses, title=\"offset=0, max_len=100\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering plots of each model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "# import PCA\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "from make_video import prepare_video\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fill_types = ('o', '8', '*', 'v', 'X', '^', '<', '>','s', 'p',  'h', 'H', 'D', 'd', 'P')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.cm as cm\n",
    "colors = cm.viridis(np.linspace(0, 1, 12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = cm.Set1.colors +  ((0.0, 0.0, 0.0), cm.Set2.colors[0], cm.tab20b.colors[-1], cm.tab20b.colors[-2], cm.tab20b.colors[-3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factors = {\n",
    "    9: (3, 3),\n",
    "    13: (4, 4),\n",
    "    5: (3, 3),\n",
    "    3: (2, 2),\n",
    "    1: (1, 1),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_clusters(folder, out_folder, reduction=\"tsne\", layer_wise=False):\n",
    "    results = get_results(os.path.join(folder, \"generations\", \"200_test.txt\"))\n",
    "    print(f\"plotting {folder}\"  + \" \" + str(len(results)))\n",
    "    os.makedirs(os.path.join(folder, \"plots\"), exist_ok=True)\n",
    "    for i in range(2):\n",
    "        example = results[i]\n",
    "        t_hidden = len(example[\"hidden_outputs\"][0])\n",
    "        t_states = len(example[\"states\"])\n",
    "        n_layers = len(example[\"hidden_outputs\"])\n",
    "        # print(f\"t_hidden: {t_hidden}, t_states: {t_states}, n_layers: {n_layers}\")\n",
    "\n",
    "        reducer = TSNE if reduction == \"tsne\" else PCA\n",
    "\n",
    "\n",
    "        if layer_wise:\n",
    "            Xs = [reducer(n_components=2).fit_transform(hidden_outputs[:t_states, :]) for hidden_outputs in example[\"hidden_outputs\"]]\n",
    "            X = np.concatenate(Xs, axis=0)\n",
    "            X = X.reshape(n_layers, t_states, 2)\n",
    "        else:\n",
    "            hidden_outputs = np.concatenate(example[\"hidden_outputs\"], axis=0)\n",
    "            hidden_outputs = hidden_outputs[:, :t_states, :]\n",
    "            X = reducer(n_components=2).fit_transform(hidden_outputs)\n",
    "            X = X.reshape(n_layers, t_states, 2)\n",
    "\n",
    "        X = X[:, :t_states, :]\n",
    "\n",
    "\n",
    "        for t in range(20, 500, 10):\n",
    "            a, _ = factors[n_layers]\n",
    "            axes, fig = plt.subplots(a, a, figsize=(16, 16))\n",
    "            # assert n_layers == 13\n",
    "            axes.suptitle(f\"T={t}, #states: \"\n",
    "                    + str(len(example[\"dfa\"].dfa._transition_function))\n",
    "                    + \", len(vocab): \"\n",
    "                    + str(len(example[\"dfa\"].dfa.alphabet)))\n",
    "\n",
    "            for layer in range(n_layers-1, -1, -1):\n",
    "                X_layer = X[layer, :t]\n",
    "                labels = np.array(example[\"states\"][:t])\n",
    "                chars = np.array(list(example[\"input\"][:t]))\n",
    "                vocab = example[\"vocab\"][1:]\n",
    "\n",
    "                ax = fig[layer // a, layer % a]\n",
    "                for label in np.unique(labels):\n",
    "                    if label == -1 or label == \"-1\":\n",
    "                        continue\n",
    "                    for ci, v in enumerate(example[\"dfa\"].dfa.alphabet):\n",
    "                        indices = np.where((labels == label) & (chars == v))\n",
    "                        if len(indices) > 0:\n",
    "                            ax.scatter(X_layer[indices, 0], X_layer[indices, 1], marker=fill_types[ci], c=colors[label])\n",
    "                            # show legend\n",
    "                # set title\n",
    "                ax.set_title(\n",
    "                    f\"layer={layer}\"\n",
    "                )\n",
    "            plot_path = os.path.join(folder, \"plots/\", f\"e_{i}_{reduction}_out_label_t_{t}.png\")\n",
    "            plt.savefig(plot_path)\n",
    "            plt.close()\n",
    "\n",
    "        video_out_folder = os.path.join(\"video_outputs\", out_folder)\n",
    "        video_glob = os.path.join(folder, \"plots/\") + \"/\" + f\"e_{i}_{reduction}_out_label_t_*.png\"\n",
    "        print(video_glob)\n",
    "        print(\"preparing video\")\n",
    "        videopath = prepare_video(video_glob, video_out_folder, f\"{i}_lw_{layer_wise}\")\n",
    "        print(f\"done at {videopath}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = get_results(exp_folders[\"transformer/8\"] + \"/generations/200_test.txt\")[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "def get_cluster_labels(example, layer=4):\n",
    "    t_hidden = len(example[\"hidden_outputs\"][0])\n",
    "    t_states = len(example[\"states\"])\n",
    "    n_layers = len(example[\"hidden_outputs\"])\n",
    "    # print(f\"t_hidden: {t_hidden}, t_states: {t_states}, n_layers: {n_layers}\")\n",
    "    reducer = TSNE\n",
    "    Xs = [reducer(n_components=2).fit_transform(hidden_outputs[:t_states, :]) for hidden_outputs in example[\"hidden_outputs\"][layer:layer+1]]\n",
    "    X = np.concatenate(Xs, axis=0)\n",
    "    X = X.reshape(1, t_states, 2)\n",
    "    X = X[:, :t_states, :]\n",
    "    X = X[0]\n",
    "    print(X.shape)\n",
    "    # find seperating hyperplane\n",
    "    kmeans = KMeans(n_clusters=2, random_state=0).fit(X)\n",
    "    labels = kmeans.labels_\n",
    "    inputs = example[\"input\"][:t_states]\n",
    "    states = example[\"states\"][:t_states]\n",
    "    return list(zip(labels, inputs, states))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_info = get_cluster_labels(data1[1], layer=4)\n",
    "\n",
    "for label, input, state in label_info:\n",
    "    print(f\"label={label} input={input} outstate={state}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = plot_clusters(exp_folders[\"transformer/8\"], \"transformer/8\", reduction=\"tsne\", layer_wise=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = []\n",
    "for exp_name, folder in exp_folders.items():\n",
    "    print(exp_name)\n",
    "    try:\n",
    "        path = plot_clusters(folder, exp_name, reduction=\"tsne\", layer_wise=True)\n",
    "        paths.append(path)\n",
    "    except:\n",
    "        print(f\"skipping {exp_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = get_results(os.path.join(exp_folders[\"transformer/8\"], \"generations\", \"200_test.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results[0][\"dfa\"].dfa._transition_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from probe import get_dfa_states\n",
    "def plot_attention_map(example, folder, output_folder, id=0):\n",
    "    t_states = len(example[\"states\"])\n",
    "    t_hidden = len(example[\"hidden_outputs\"][0])\n",
    "    t_states = len(example[\"states\"])\n",
    "    n_layers = len(example[\"hidden_outputs\"])\n",
    "    in_states = get_dfa_states(example[\"input\"], example[\"dfa\"], in_states=True)\n",
    "    out_states = get_dfa_states(example[\"input\"], example[\"dfa\"], in_states=False)\n",
    "\n",
    "    start = np.random.randint(0, t_states)\n",
    "    end = min(start + 20, t_states)\n",
    "\n",
    "    start = 0\n",
    "    end = 80\n",
    "\n",
    "\n",
    "    chars = list(example[\"input\"])[start: end]\n",
    "    print(chars)\n",
    "    in_states = in_states[start: end]\n",
    "    out_states = out_states[start: end]\n",
    "    chars = list(zip(in_states, chars, out_states))\n",
    "\n",
    "    os.makedirs(os.path.join(folder, \"attention_plots\"), exist_ok=True)\n",
    "\n",
    "    for layer in range(8):\n",
    "        print(layer)\n",
    "        attentions = example[\"attention_scores\"][layer][:, start:end, start:end] # 2 x T X T\n",
    "        print(attentions.shape)\n",
    "        fig, ax = plt.subplots(2, 1, figsize=(25, 25))\n",
    "        # plot heatmap of attentions per head\n",
    "        # use sns plot\n",
    "        for head in range(attentions.shape[0]):\n",
    "            sns.heatmap(attentions[head],\n",
    "                        ax=ax[head],\n",
    "                        xticklabels=chars,\n",
    "                        yticklabels=chars,\n",
    "                        annot=False)\n",
    "            ax[head].set_title(f\"layer={layer}, head={head}\")\n",
    "            ax[head].xaxis.tick_top()\n",
    "            # make xticks rotated 90 degrees\n",
    "            plt.setp(ax[head].get_xticklabels(), rotation=90)\n",
    "            # save figure\n",
    "        os.makedirs(os.path.join(\"attention_plots\", output_folder, f\"{id}\"), exist_ok=True)\n",
    "        path = os.path.join(\"attention_plots\", output_folder, f\"{id}/layer_{layer}_head_{head}.jpeg\")\n",
    "        plt.savefig(path)\n",
    "            # ax[head].set_xticks(range(len(chars)))\n",
    "            # ax[head].set_xticklabels(chars)\n",
    "            # ax[head].set_yticks(range(len(chars)))\n",
    "            # ax[head].set_yticklabels(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_attention_map(results[0], exp_folders[\"transformer/8\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = []\n",
    "for exp_name, folder in exp_folders.items():\n",
    "    print(exp_name)\n",
    "    try:\n",
    "        results = get_results(os.path.join(folder, \"generations\", \"200_test.txt\"))\n",
    "        # plot_attention_map(results[0], folder, output_folder=exp_name, id=0)\n",
    "        plot_attention_map(results[1], folder, output_folder=exp_name, id=1)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(f\"skipping {exp_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_folders[\"transformer/8\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results[0][\"attention\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from probe import get_dfa_states\n",
    "from bertviz import head_view, model_view\n",
    "import torch\n",
    "\n",
    "def visualize_attention_map(example, folder, id=0, output_folder=\"transformer/8\"):\n",
    "    t_states = len(example[\"states\"])\n",
    "    t_hidden = len(example[\"hidden_outputs\"][0])\n",
    "    t_states = len(example[\"states\"])\n",
    "    n_layers = len(example[\"hidden_outputs\"])\n",
    "    in_states = get_dfa_states(example[\"input\"], example[\"dfa\"], in_states=True)\n",
    "    out_states = get_dfa_states(example[\"input\"], example[\"dfa\"], in_states=False)\n",
    "\n",
    "    start = np.random.randint(0, t_states)\n",
    "    end = min(start + 20, t_states)\n",
    "\n",
    "    start = 0\n",
    "    end = 80\n",
    "\n",
    "\n",
    "    chars = list(example[\"input\"])[start: end]\n",
    "    print(chars)\n",
    "    in_states = in_states[start: end]\n",
    "    out_states = out_states[start: end]\n",
    "    chars = list(zip(in_states, chars, out_states))\n",
    "    chars = [str(text) for text in chars]\n",
    "    attentions = [torch.tensor(scores[None, :, start:end, start:end]) for scores in example[\"attention_scores\"]]\n",
    "    html_head_view = head_view(attentions, chars, html_action='return')\n",
    "\n",
    "    output_folder = os.path.join(\"attention_plots\", output_folder)\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    with open(os.path.join(output_folder, f\"{id}.html\"), 'w') as file:\n",
    "        file.write(html_head_view.data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = []\n",
    "for exp_name, folder in exp_folders.items():\n",
    "    print(exp_name)\n",
    "    try:\n",
    "        results = get_results(os.path.join(folder, \"generations\", \"200_test.txt\"))\n",
    "        visualize_attention_map(results[0], folder, id=0, output_folder=exp_name)\n",
    "        visualize_attention_map(results[1], folder, id=1, output_folder=exp_name)\n",
    "    except:\n",
    "        print(f\"skipping {exp_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_attention_map(results[0], exp_folders[\"transformer/8\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html_head_view = head_view(attention, tokens, html_action='return')\n",
    "\n",
    "with open(\"PATH_TO_YOUR_FILE/head_view.html\", 'w') as file:\n",
    "    file.write(html_head_view.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(results[0][\"attention_scores\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iclmodels",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
