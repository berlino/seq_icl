{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# plot heatmap\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from analyze import get_dfa_probs as calculate_dfa_probs\n",
    "from ngram import (\n",
    "    predict_with_n_gram_back_off,\n",
    "    prob_distance,\n",
    "    prob_distance_dfa,\n",
    "    prob_distance_dfa_ngram,\n",
    ")\n",
    "\n",
    "from batched_baum_welch import predict_with_baumwelch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab:\n",
    "    def __init__(self, vocab: list):\n",
    "        self.vocab = vocab\n",
    "        # inverse vocab\n",
    "        self.inv_vocab = {v: k for k, v in enumerate(vocab)}\n",
    "\n",
    "    def get_vocab(self, id):\n",
    "        return self.vocab[id]\n",
    "\n",
    "    def get_id(self, char):\n",
    "        return self.inv_vocab[char]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.vocab)\n",
    "\n",
    "\n",
    "def get_ngram_probs(results, ngram=3, uniform=False, backoff=False, addone=False):\n",
    "    vocab = Vocab(results[0][\"vocab\"])\n",
    "    n_gram_probs = []\n",
    "    for b in range(len(results)):\n",
    "        input = results[b][\"input\"]\n",
    "        target = [vocab.get_id(t) for t in results[b][\"target\"]]\n",
    "        probs = predict_with_n_gram_back_off(\n",
    "            input,\n",
    "            N=ngram,\n",
    "            global_vocab=vocab,\n",
    "            uniform=uniform,\n",
    "            backoff=backoff,\n",
    "            addone=addone,\n",
    "        )\n",
    "        n_gram_probs.append(probs)\n",
    "    return n_gram_probs\n",
    "\n",
    "\n",
    "def get_baumwelch_probs(results):\n",
    "    vocab = Vocab(results[0][\"vocab\"])\n",
    "    baumwelch_probs = []\n",
    "    for b in range(len(results)):\n",
    "        input = results[b][\"input\"]\n",
    "        probs = predict_with_baumwelch(input, vocab, max_states=12)\n",
    "        baumwelch_probs.append(probs)\n",
    "    return baumwelch_probs\n",
    "\n",
    "\n",
    "def get_dfa_probs(results):\n",
    "    vocab = Vocab(results[0][\"vocab\"])\n",
    "    dfa_probs = []\n",
    "    for b in range(len(results)):\n",
    "        input = results[b][\"input\"]\n",
    "        target = [vocab.get_id(t) for t in results[b][\"target\"]]\n",
    "        probs = calculate_dfa_probs(input, results[b][\"dfa\"], vocab=vocab)\n",
    "        dfa_probs.append(probs)\n",
    "    return dfa_probs\n",
    "\n",
    "\n",
    "def get_model_probs(results, softmax=True):\n",
    "    model_probs = []\n",
    "    for b in range(len(results)):\n",
    "        if softmax:\n",
    "            probs = (\n",
    "                torch.softmax(torch.tensor(results[b][\"probs\"]), dim=-1)\n",
    "                .detach()\n",
    "                .cpu()\n",
    "                .numpy()\n",
    "            )\n",
    "        else:\n",
    "            probs = results[b][\"probs\"]\n",
    "        model_probs.append(probs)\n",
    "    return model_probs\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def get_greedy_dfa_accuracy(probs, dfa_probs, offset=0, max_len=None):\n",
    "    total = 0.0\n",
    "    correct = 0.0\n",
    "    for p1, pdfa in zip(probs, dfa_probs):\n",
    "        if max_len is not None:\n",
    "            pdfa = pdfa[offset:max_len]\n",
    "        indices = p1.argmax(axis=-1)[: len(pdfa)]\n",
    "        correct += (pdfa[np.arange(len(pdfa)), indices] > 0).sum()\n",
    "        total += len(pdfa)\n",
    "    return correct / total\n",
    "\n",
    "\n",
    "EPS = 1e-7\n",
    "\n",
    "\n",
    "def get_kl(probs, dfa_probs, offset=0, max_len=None):\n",
    "    total = 0.0\n",
    "    cross_entropy = 0.0\n",
    "    for p1, pdfa in zip(probs, dfa_probs):\n",
    "        # calculate the soft cross-entropy between p1 and pdfa\n",
    "        if max_len is not None:\n",
    "            pdfa = pdfa[offset:max_len]\n",
    "        log_p1 = np.log(p1[: len(pdfa)] + EPS)\n",
    "        log_pdfa = np.log(pdfa + EPS)\n",
    "        cross_entropy += -((log_p1 - log_pdfa) * pdfa).sum()\n",
    "        total += len(pdfa)\n",
    "    return cross_entropy / total\n",
    "\n",
    "\n",
    "def get_l1_loss(probs1, probs2, probsdfa, offset=0, max_len=None):\n",
    "    total = 0.0\n",
    "    correct = 0.0\n",
    "    for p1, p2, pdfa in zip(probs1, probs2, probsdfa):\n",
    "        if max_len is not None:\n",
    "            pdfa = pdfa[offset:max_len]\n",
    "        total += len(pdfa)\n",
    "        correct += np.abs(\n",
    "            p1[offset : offset + len(pdfa)] - p2[offset : offset + len(pdfa)]\n",
    "        ).sum()\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# glob all checkpoints\n",
    "run_folders = glob.glob(\n",
    "    \"experiments/hiddens_*/**/generations/*test_batch/\", recursive=True\n",
    ")\n",
    "# create a map\n",
    "name_to_folder = {}\n",
    "for folder in run_folders:\n",
    "    folder = folder.replace(\"//\", \"/\").strip(\"/\")\n",
    "    subpaths = folder.split(\"/\")\n",
    "    name = subpaths[2]\n",
    "    num_examples = subpaths[1].split(\"_\")[1]\n",
    "    nlayer = \"\" if subpaths[3] == \"generations\" else subpaths[3]\n",
    "    name = f\"{num_examples}/{name}/{nlayer}\"\n",
    "    name_to_folder[name] = folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_to_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import functools\n",
    "# import concurrent\n",
    "# import pickle\n",
    "# def read_one(fname, probs_only=False):\n",
    "#     with open(fname, \"rb\") as f:\n",
    "#         data =  pickle.load(f)\n",
    "#         if \"hidden_outputs\" in data:\n",
    "#             del data[\"hidden_outputs\"]\n",
    "#         if \"attention_scores\" in data:\n",
    "#             del data[\"attention_scores\"]\n",
    "#         if \"attention_contexts\" in data:\n",
    "#             del data[\"attention_contexts\"]\n",
    "\n",
    "#     with open(fname, 'wb') as f:\n",
    "#         pickle.dump(data, f)\n",
    "\n",
    "#     return data\n",
    "\n",
    "\n",
    "# def read_parallel(file_names, probs_only=False):\n",
    "#     reader = functools.partial(read_one, probs_only=probs_only)\n",
    "#     with concurrent.futures.ThreadPoolExecutor(max_workers=16) as executor:\n",
    "#         futures = [executor.submit(reader, f) for f in file_names]\n",
    "#         return [fut.result() for fut in futures]\n",
    "\n",
    "# for model in name_to_folder.keys():\n",
    "#     if model.startswith(\"20000\"):\n",
    "#         print(model)\n",
    "#         folder = name_to_folder[model]\n",
    "#         files = glob.glob(f\"{folder}/*.pkl\")\n",
    "#         print(files[0])\n",
    "#         read_parallel(files, probs_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from probe import get_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "for num_examples in (1000, 2500, 5000, 10000, 20000, 40000):\n",
    "    results[num_examples] = {}\n",
    "    for model in (\n",
    "        \"transformer\",\n",
    "        \"lstm\",\n",
    "        \"hyena\",\n",
    "        \"h3\",\n",
    "        \"s4d\",\n",
    "        \"linear_transformer\",\n",
    "        \"rwkv\",\n",
    "        \"retention\",\n",
    "        \"transformer_4\",\n",
    "        \"transformer_8\",\n",
    "        \"transformer_2\",\n",
    "        \"transformer_1\",\n",
    "    ):\n",
    "        name = f\"{num_examples}/{model}/\"\n",
    "        if model in results[num_examples]:\n",
    "            continue\n",
    "        try:\n",
    "            results[num_examples][model] = get_results(\n",
    "                name_to_folder[name], probs_only=True\n",
    "            )\n",
    "        except:\n",
    "            print(f\"Failed to load {name}\")\n",
    "    for model in (\"2gram\", \"3gram\"):\n",
    "        file = f\"experiments/hiddens_{num_examples}/{model}/probs.pkl\"\n",
    "        try:\n",
    "            results[num_examples][model] = pickle.load(open(file, \"rb\"))\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(f\"Failed to load {file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for num_examples in (1000, 2500, 5000, 10000, 20000, 40000):\n",
    "    results[num_examples][\"dfa\"] = get_dfa_probs(results[num_examples][\"transformer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results[2500][\"2gram\"][0]['probs'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_ngram_probs(\n",
    "        results[1000][\"transformer\"],\n",
    "        ngram=3,\n",
    "        uniform=False,\n",
    "        backoff=True,\n",
    "        addone=False,\n",
    "    ).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gram2 = get_ngram_probs(\n",
    "        results[1000][\"transformer\"],\n",
    "        ngram=2,\n",
    "        uniform=False,\n",
    "        backoff=True,\n",
    "        addone=False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gram2[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for num_examples in (1000, 2500, 5000, 10000, 20000, 40000):\n",
    "    results[num_examples][\"3gram\"] = get_ngram_probs(\n",
    "        results[num_examples][\"transformer\"],\n",
    "        ngram=3,\n",
    "        uniform=False,\n",
    "        backoff=True,\n",
    "        addone=False,\n",
    "    )\n",
    "    results[num_examples][\"2gram\"] = get_ngram_probs(\n",
    "        results[num_examples][\"transformer\"],\n",
    "        ngram=2,\n",
    "        uniform=False,\n",
    "        backoff=True,\n",
    "        addone=False,\n",
    "    )\n",
    "    folder = f\"experiments/hiddens_{num_examples}/\"\n",
    "    for algo in (\"2gram\", \"3gram\"):\n",
    "        probs = results[num_examples][algo]\n",
    "        infos = results[num_examples][\"transformer\"]\n",
    "        data = [\n",
    "            {\n",
    "                \"probs\": p,\n",
    "                \"dfa\": d[\"dfa\"],\n",
    "                \"input\": d[\"input\"],\n",
    "                \"vocab\": d[\"vocab\"],\n",
    "            }\n",
    "            for p, d in zip(probs, infos)\n",
    "        ]\n",
    "        os.makedirs(f\"{folder}/{algo}\", exist_ok=True)\n",
    "        with open(f\"{folder}/{algo}/probs.pkl\", \"wb\") as f:\n",
    "            pickle.dump(data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "\n",
    "# for num_examples in (2500,):\n",
    "#     # save ngrams to a folder\n",
    "#     folder = f\"experiments/hiddens_{num_examples}/\"\n",
    "#     for algo in (\"2gram\", \"3gram\"):\n",
    "#         # makedir\n",
    "#         probs = results[num_examples][algo]\n",
    "#         infos = results[num_examples][\"transformer\"]\n",
    "#         data = [\n",
    "#             {\n",
    "#                 \"probs\": p,\n",
    "#                 \"dfa\": d[\"dfa\"],\n",
    "#                 \"input\": d[\"input\"],\n",
    "#                 \"vocab\": d[\"vocab\"],\n",
    "#             }\n",
    "#             for p, d in zip(probs, infos)\n",
    "#         ]\n",
    "#         os.makedirs(f\"{folder}/{algo}\", exist_ok=True)\n",
    "#         with open(f\"{folder}/{algo}/probs.pkl\", \"wb\") as f:\n",
    "#             pickle.dump(data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
    "\n",
    "\n",
    "def plot_l1_table(results, num_examples=20000, offset=0, max_len=None):\n",
    "    models = [\n",
    "        \"dfa\",\n",
    "        \"2gram\",\n",
    "        \"3gram\",\n",
    "        \"hyena\",\n",
    "        \"linear_transformer\",\n",
    "        \"transformer\",\n",
    "        \"lstm\",\n",
    "        \"transformer_2\",\n",
    "        \"transformer_4\",\n",
    "    ]  # , \"transformer_1\", \"transformer_2\", \"transformer_4\", \"transformer_8\", \"lstm\",]\n",
    "\n",
    "    models = set(models).intersection(list(results[num_examples].keys()))\n",
    "    # L1 Differences\n",
    "    l1_table = []\n",
    "    dfa_probs = results[num_examples][\"dfa\"]\n",
    "    for model1 in models:\n",
    "        if model1 not in (\"dfa\",):\n",
    "            model1_probs = get_model_probs(results[num_examples][model1], softmax=\"gram\" not in model1)\n",
    "        else:\n",
    "            model1_probs = results[num_examples][model1]\n",
    "        for model2 in models:\n",
    "            if model2 not in (\"dfa\",):\n",
    "                model2_probs = get_model_probs(results[num_examples][model2], softmax=\"gram\" not in model2)\n",
    "            else:\n",
    "                model2_probs = results[num_examples][model2]\n",
    "            value = (\n",
    "                get_l1_loss(\n",
    "                    model1_probs,\n",
    "                    model2_probs,\n",
    "                    dfa_probs,\n",
    "                    offset=offset,\n",
    "                    max_len=max_len,\n",
    "                )\n",
    "                / 2\n",
    "            )\n",
    "\n",
    "            model1 = (\n",
    "                model1.replace(\"transformer\", \"TF\")\n",
    "                .replace(\"linear_\", \"L\")\n",
    "                .replace(\"_\", \"/\")\n",
    "            )\n",
    "            model2 = (\n",
    "                model2.replace(\"transformer\", \"TF\")\n",
    "                .replace(\"linear_\", \"L\")\n",
    "                .replace(\"_\", \"/\")\n",
    "            )\n",
    "\n",
    "            l1_table.append([model1, model2, value])\n",
    "\n",
    "    l1_df = pd.DataFrame(l1_table, columns=[\"model1\", \"model2\", \"value\"])\n",
    "\n",
    "    l1_df = l1_df.set_index([\"model1\"]).pivot(columns=\"model2\", values=\"value\")\n",
    "\n",
    "    # fig size\n",
    "    matplotlib.rcParams[\"figure.figsize\"] = (6, 6)\n",
    "    matplotlib.rcParams[\"font.size\"] = 8\n",
    "    matplotlib.rcParams[\"font.family\"] = \"serif\"\n",
    "    matplotlib.rcParams[\"figure.dpi\"] = 300\n",
    "    fix, ax = plt.subplots()\n",
    "    sns.heatmap(\n",
    "        l1_df, annot=True, ax=ax, cbar_kws={\"orientation\": \"horizontal\", \"pad\": 0.01}\n",
    "    )\n",
    "    # ax.set_title(f\"L1 Loss (N={num_examples})\")\n",
    "    ax.xaxis.tick_top()\n",
    "    ax.xaxis.set_label_position(\"top\")\n",
    "    ax.set_xlabel(f\"Model-1\")\n",
    "    ax.set_ylabel(\"Model-2\")\n",
    "    ax.xaxis.set_ticks_position(\"none\")\n",
    "    ax.yaxis.set_ticks_position(\"none\")\n",
    "    plt.xticks(rotation=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_l1_table(results, num_examples=2500, offset=0, max_len=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_l1_table(results, num_examples=40000, offset=0, max_len=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_l1_table(results, num_examples=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hard_indices = {}\n",
    "for num_examples in (1000, 2500, 5000, 10000, 20000, 40000):\n",
    "    hard_indices[num_examples] = [\n",
    "        ind\n",
    "        for ind, r in enumerate(results[num_examples][\"transformer\"])\n",
    "        if len(r[\"dfa\"].dfa._states) > 7 and len(r[\"dfa\"].dfa.alphabet) > 10\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfa_metrics = []\n",
    "for metric in (\"l1\", \"kl\", \"acc\"):\n",
    "    for num_examples in (1000, 2500, 5000, 10000, 20000, 40000):\n",
    "        for model in results[num_examples].keys():\n",
    "            if model != \"dfa\":\n",
    "                if model not in (\"3gram\", \"2gram\"):\n",
    "                    model_probs = get_model_probs(results[num_examples][model])\n",
    "                else:\n",
    "                    model_probs = results[num_examples][model]\n",
    "                dfa_probs = results[num_examples][\"dfa\"]\n",
    "                # # hard\n",
    "                # model_probs = [model_probs[i] for i in hard_indices[num_examples]]\n",
    "                # dfa_probs = [dfa_probs[i] for i in hard_indices[num_examples]]\n",
    "\n",
    "                if metric == \"l1\":\n",
    "                    value = get_l1_loss(model_probs, dfa_probs, dfa_probs) / 2\n",
    "                elif metric == \"kl\":\n",
    "                    value = get_kl(model_probs, dfa_probs)\n",
    "                elif metric == \"acc\":\n",
    "                    value = get_greedy_dfa_accuracy(model_probs, dfa_probs)\n",
    "                dfa_metrics.append(\n",
    "                    {\n",
    "                        \"model\": model,\n",
    "                        \"metric\": metric,\n",
    "                        \"value\": value,\n",
    "                        \"num_examples\": num_examples,\n",
    "                    }\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deep json to dataframe\n",
    "df = pd.DataFrame(dfa_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make neurips conference quality plots\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.style.use(\"/raid/lingo/akyurek/mplstyle\")\n",
    "plt.rc(\"font\", serif=\"Times\")\n",
    "plt.rc(\"text\", usetex=False)\n",
    "plt.rcParams[\"figure.dpi\"] = 250\n",
    "plt.rcParams[\"figure.facecolor\"] = \"white\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_names = {\n",
    "    \"kl\": \"KL\",\n",
    "    \"l1\": \"TVD\",\n",
    "    \"acc\": \"Accuracy\",\n",
    "}\n",
    "\n",
    "model_names = {\n",
    "    \"transformer\": \"Transformer\",\n",
    "    \"transformer_2\": \"Transformer (2 layers)\",\n",
    "    \"transformer_1\": \"Transformer (1 layers)\",\n",
    "    \"lstm\": \"LSTM\",\n",
    "    \"hyena\": \"Hyena\",\n",
    "    \"h3\": \"H3\",\n",
    "    \"s4d\": \"S4D\",\n",
    "    \"linear_transformer\": \"Linear Transformer\",\n",
    "    \"rwkv\": \"RWKV\",\n",
    "    \"retention\": \"RetNet\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig size\n",
    "plt.rcParams.update({\"figure.figsize\": (6, 4)})\n",
    "metric = \"l1\"\n",
    "\n",
    "data = df[\n",
    "    (df.metric == metric)\n",
    "    & ~(\n",
    "        df.model.isin(\n",
    "            [\n",
    "                \"transformer_8\",\n",
    "                \"transformer_4\",\n",
    "                \"transformer_2\",\n",
    "                \"transformer_1\",\n",
    "                \"3gram\",\n",
    "                \"2gram\",\n",
    "            ]\n",
    "        )\n",
    "    )\n",
    "]\n",
    "data = data.replace({\"model\": model_names})\n",
    "ax = sns.lineplot(\n",
    "    data=data,\n",
    "    x=\"num_examples\",\n",
    "    y=\"value\",\n",
    "    hue=\"model\",\n",
    "    marker=\"o\",\n",
    "    linewidth=1.5,\n",
    ")\n",
    "ax.set_xlabel(\"# Training Examples\")\n",
    "ax.set_ylabel(metric_names[metric])\n",
    "ax.set(xscale=\"log\")\n",
    "ax.set_xticks([1000, 2500, 5000, 10000, 20000, 40000])\n",
    "ax.legend(title=\"Model\")\n",
    "ax.get_xaxis().set_major_formatter(matplotlib.ticker.ScalarFormatter())\n",
    "# show 2gram and 3gram as horizontal line with texts on them\n",
    "ax.axhline(\n",
    "    y=df[(df.metric == metric) & (df.model == \"3gram\")].value.mean(),\n",
    "    color=\"black\",\n",
    "    linestyle=\"--\",\n",
    "    linewidth=0.5,\n",
    ")\n",
    "ax.text(\n",
    "    1000,\n",
    "    df[(df.metric == metric) & (df.model == \"3gram\")].value.mean() + 0.02,\n",
    "    \"3-gram\",\n",
    "    color=\"black\",\n",
    ")\n",
    "ax.axhline(\n",
    "    y=df[(df.metric == metric) & (df.model == \"2gram\")].value.mean(),\n",
    "    color=\"gray\",\n",
    "    linestyle=\"--\",\n",
    "    linewidth=0.5,\n",
    ")\n",
    "ax.text(\n",
    "    1000,\n",
    "    df[(df.metric == metric) & (df.model == \"2gram\")].value.mean() + 0.03,\n",
    "    \"2-gram\",\n",
    "    color=\"gray\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iclmodels",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
