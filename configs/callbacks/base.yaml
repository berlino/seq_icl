learning_rate_monitor:
  # _target_: pytorch_lightning.callbacks.LearningRateMonitor
  logging_interval: ${train.interval}

timer:
  # _target_: callbacks.timer.Timer
  step: True
  inter_step: False
  epoch: True
  val: True

params:
  # _target_: callbacks.params.ParamsLog
  total: True
  trainable: True
  fixed: True

model_checkpoint:
  # _target_: pytorch_lightning.callbacks.ModelCheckpoint
  monitor: ${train.monitor} # name of the logged metric which determines when model is improving
  mode: ${train.mode} # can be "max" or "min"
  save_top_k: 1 # save k best models (determined by above metric)
  save_last: False # additionaly always save model from last epoch
  dirpath: "checkpoints/"
  # this saves an annoying "epoch=12" which makes it annoying to pass on the command line; epochs are being logged through the verbose flag and logger anyways
  # seems like you can override the '.format_checkpoint_name' method of ModelCheckpoint to change this, but not worth
  # filename: "{epoch:02d}",
  filename: ${train.monitor}
  auto_insert_metric_name: False
  verbose: True
