# @package _global_
defaults:
  - /trainer: default
  - /loader: default
  - /dataset: icl_dfa
  - /optimizer: adamw
  - /scheduler: cosine_warmup
  - /callbacks: base

dataset:
  vocab_size: 20

train:
  monitor: val/loss
  mode: min

task:
  _name_: dfalm
  loss: cross_entropy
  torchmetrics: ['perplexity']
  metrics: ['accuracy_ignore_index']

encoder: null
decoder: null
