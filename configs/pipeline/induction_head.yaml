# @package _global_
defaults:
  - /trainer: default
  - /loader: default
  - /dataset: icl_synthetics
  - /optimizer: adamw
  - /scheduler: cosine_warmup
  - /callbacks: base

dataset:
  vocab_size: 16
  input_seq_len: 256
  copy_method: "induction_head"

train:
  monitor: val/accuracy_ignore_index
  mode: max

task:
  _name_: dfalm
  loss: cross_entropy
  torchmetrics: ['perplexity']
  metrics: ['accuracy_ignore_index']

encoder: null
decoder: null